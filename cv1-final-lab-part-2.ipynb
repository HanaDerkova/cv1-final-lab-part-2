{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1200/1*lbDXL0IuitCRz4mpZ7MmfQ.png\" width=55% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Final Lab (Part 2): Image Classification using Convolutional Neural Networks </font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, 24th October, 2025 (Amsterdam time)</font> \n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID: 11977477 \\\n",
    "Student1 Name: Cinthya Criollo \n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name: \n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name: \n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coding Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab: Image Classification Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification. \n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on performance (accuracy, learning loss and learning curves). \n",
    "\n",
    "4. **Aim:** Understand the basic Image Classification pipeline using Convolutional Neural Nets (CNN's).\n",
    "\n",
    "5. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part2.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "6. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your model(s). Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Image Classification on CIFAR-100 (0 points)](#section-1)\n",
    "- [Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)](#section-2)\n",
    "- [Section 3: TwoLayerNet Architecture (2 points)](#section-3)\n",
    "- [Section 4: ConvNet Architecture (2 points)](#section-4)\n",
    "- [Section 5: Preparation of Training (7 points)](#section-5)\n",
    "- [Section 6: Training the Networks (5 points)](#section-6)\n",
    "- [Section 7: Setting Up the Hyperparameters (14 points)](#section-7)\n",
    "- [Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)](#section-8)\n",
    "- [Section 9: Fine-tuning ConvNet on STL-10 (14 points)](#section-9)\n",
    "- [Section 10: Bonus Challenge (optional)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Image Classification on CIFAR-100 (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system using Convolutional Neural Networks (CNNs) that can identify objects from a set of classes in the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). You will implement and compare two different architectures: a simple two-layer network and a ConvNet based on the LeNet architecture.\n",
    "\n",
    "The CIFAR-100 dataset contains 32x32 pixel RGB images, categorized into 100 different classes. The dataset will be automatically downloaded and loaded using the code provided in this notebook.\n",
    "\n",
    "You will train and test your classification system using the entire CIFAR-100 dataset. Ensure that the test images are excluded from training to maintain a fair evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:19<00:00, 8771453.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Data loaders for CIFAR-100 are ready for use.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the CIFAR-100 test set\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for the entire CIFAR-100 dataset\n",
    "train_data_loader = DataLoader(train_set, shuffle=True)\n",
    "test_data_loader = DataLoader(test_set, shuffle=False)\n",
    "\n",
    "# Define CIFAR-100 superclasses and their subclasses\n",
    "superclasses = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "    'household electrical devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "# List of all CIFAR-100 classes\n",
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', \n",
    "           'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "           'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "           'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "           'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "           'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
    "           'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', \n",
    "           'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', \n",
    "           'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
    "\n",
    "# Create a mapping of class names to their indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "# Create a mapping of superclasses to their corresponding class indices\n",
    "superclass_to_indices = {supcls: [class_to_idx[cls] for cls in subclasses] for supcls, subclasses in superclasses.items()}\n",
    "\n",
    "print(\"Data loaders for CIFAR-100 are ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)**\n",
    "\n",
    "In this section, you will implement a function to visualize the CIFAR-100 dataset, including **all** superclasses and their corresponding subclasses. Your implementation should provide a clear and organized overview of the dataset's diversity.\n",
    "\n",
    "You add the figure(s) to appendix of your report and refer to it in the main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20 figures to: c:\\Users\\cinth\\Documentos\\ams\\ai\\cv1\\cv1-final-lab-part-2\\figures_cifar100_superclasses\n",
      "Saved montage to: c:\\Users\\cinth\\Documentos\\ams\\ai\\cv1\\cv1-final-lab-part-2\\figures_cifar100_superclasses\\cifar100_10x10_montage.png\n",
      "All visualization figures are ready. Add them to your appendix and cite them in your report.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 1) Build an index of dataset positions per class id (fast sampling later)\n",
    "def build_class_index_map(dataset):\n",
    "    by_cls = {}\n",
    "    # torchvision CIFAR100 returns (img, target) with .targets list\n",
    "    for idx, y in enumerate(dataset.targets):\n",
    "        by_cls.setdefault(y, []).append(idx)\n",
    "    return by_cls\n",
    "\n",
    "# 2) Un-normalize tensor image that was normalized with mean/std=(0.5,0.5,0.5)\n",
    "def unnormalize(img_t):\n",
    "    # img_t: CxHxW in [-1,1] because of Normalize((.5,.5,.5), (.5,.5,.5))\n",
    "    return img_t.mul(0.5).add(0.5).clamp(0, 1)\n",
    "\n",
    "# 3) Sample k images (as tensors) from a subclass name\n",
    "def sample_images_for_class(dataset, class_name, class_to_idx, index_map, k=5, seed=0):\n",
    "    rnd = random.Random(seed)\n",
    "    cls_id = class_to_idx[class_name]\n",
    "    pool = index_map[cls_id]\n",
    "    idxs = pool if len(pool) <= k else rnd.sample(pool, k)\n",
    "    imgs = []\n",
    "    for i in idxs:\n",
    "        img_t, _ = dataset[i]  # already transformed (normalized tensor)\n",
    "        imgs.append(unnormalize(img_t).permute(1, 2, 0))  # HxWxC in [0,1]\n",
    "    return imgs\n",
    "\n",
    "# 4) One figure per superclass: rows=subclasses, cols=samples_per_class\n",
    "def visualize_superclasses_grid(\n",
    "    dataset,\n",
    "    superclasses_dict,\n",
    "    class_to_idx,\n",
    "    samples_per_class=5,\n",
    "    out_dir=\"figures_cifar100_superclasses\",\n",
    "    dpi=160,\n",
    "    seed=0,\n",
    "    show=False\n",
    "):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    index_map = build_class_index_map(dataset)\n",
    "    saved_paths = []\n",
    "\n",
    "    for sup, subs in superclasses_dict.items():\n",
    "        rows = len(subs)\n",
    "        cols = samples_per_class\n",
    "        fig_h = max(2.5, 0.9 * rows)    # scale gently with rows\n",
    "        fig_w = max(4.5, 1.2 * cols)    # scale gently with cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(fig_w, fig_h), squeeze=False)\n",
    "        fig.suptitle(f\"{sup}\", fontsize=12, y=0.995)\n",
    "\n",
    "        for r, sub in enumerate(subs):\n",
    "            imgs = sample_images_for_class(dataset, sub, class_to_idx, index_map,\n",
    "                                           k=samples_per_class, seed=seed+hash(sub)%10000)\n",
    "            for c in range(cols):\n",
    "                ax = axes[r][c]\n",
    "                ax.axis(\"off\")\n",
    "                if c < len(imgs):\n",
    "                    ax.imshow(imgs[c].cpu())\n",
    "                if c == 0:\n",
    "                    ax.set_title(sub.replace('_',' '), fontsize=9, loc='left', pad=2)\n",
    "\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        out_path = os.path.join(out_dir, f\"{sup.replace(' ','_')}.png\")\n",
    "        fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close(fig)\n",
    "        saved_paths.append(out_path)\n",
    "\n",
    "    print(f\"Saved {len(saved_paths)} figures to: {os.path.abspath(out_dir)}\")\n",
    "    return saved_paths\n",
    "\n",
    "# 5) Single 10x10 montage of ALL 100 classes (1 sample per class)\n",
    "def visualize_all_classes_montage(\n",
    "    dataset,\n",
    "    classes_list,\n",
    "    class_to_idx,\n",
    "    out_path=\"figures_cifar100_superclasses/cifar100_10x10_montage.png\",\n",
    "    grid_rows=10,\n",
    "    grid_cols=10,\n",
    "    dpi=200,\n",
    "    seed=123\n",
    "):\n",
    "    assert len(classes_list) == grid_rows * grid_cols, \"Need 100 classes for a 10x10 grid.\"\n",
    "    Path(os.path.dirname(out_path)).mkdir(parents=True, exist_ok=True)\n",
    "    index_map = build_class_index_map(dataset)\n",
    "    rnd = random.Random(seed)\n",
    "\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(12, 12))\n",
    "    for i, cls_name in enumerate(classes_list):\n",
    "        r, c = divmod(i, grid_cols)\n",
    "        ax = axes[r][c]\n",
    "        ax.axis(\"off\")\n",
    "        cls_id = class_to_idx[cls_name]\n",
    "        pool = index_map[cls_id]\n",
    "        idx = rnd.choice(pool)\n",
    "        img_t, _ = dataset[idx]\n",
    "        img = unnormalize(img_t).permute(1, 2, 0)  # HxWxC\n",
    "        ax.imshow(img.cpu())\n",
    "        ax.set_title(cls_name.replace('_',' '), fontsize=7, pad=1)\n",
    "\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved montage to: {os.path.abspath(out_path)}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# (A) One figure per superclass with 4 examples per subclass:\n",
    "_ = visualize_superclasses_grid(\n",
    "    dataset=train_set,                         # or test_set\n",
    "    superclasses_dict=superclasses,\n",
    "    class_to_idx=class_to_idx,\n",
    "    samples_per_class=4,\n",
    "    out_dir=\"figures_cifar100_superclasses\",\n",
    "    dpi=160,\n",
    "    seed=0,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "# (B) A single 10x10 montage (one sample per fine class):\n",
    "_ = visualize_all_classes_montage(\n",
    "    dataset=train_set,\n",
    "    classes_list=list(classes),               # tuple -> list\n",
    "    class_to_idx=class_to_idx,\n",
    "    out_path=\"figures_cifar100_superclasses/cifar100_10x10_montage.png\",\n",
    "    grid_rows=10, grid_cols=10, dpi=200, seed=123\n",
    ")\n",
    "\n",
    "print(\"All visualization figures are ready. Add them to your appendix and cite them in your report.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: TwoLayerNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement the architecture of a fully connected neural network called `TwoLayerNet`, consisting of two fully connected layers with a ReLU activation in between. The network accepts an input size of 3x32x32 (CIFAR-100 image), a specified hidden layer size, and the number of output classes. In the `__init__` method, define the first fully connected layer that maps the input size to the hidden size, and the second fully connected layer that maps the hidden size to the number of classes. \n",
    "\n",
    "Ensure to call the parent class constructor using `super(TwoLayerNet, self).__init__()`. In the `forward` method, flatten the input tensor, pass it through the first layer with ReLU activation, and then through the second layer to obtain the final scores.\n",
    "\n",
    "**Note:** You are allowed to modify the provided function definitions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "    \n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Flatten input: (N, 3, 32, 32) -> (N, 3072)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # First layer + ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Output layer (no activation, scores only)\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: ConvNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement a convolutional neural network inspired by the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791). The network processes color images using three convolutional layers followed by two fully connected layers. Since you need to feed color images into this network, determine the kernel size of the first convolutional layer. Additionally, calculate the number of trainable parameters in the \"F6\" layer, providing the calculation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\t\n",
    "        Initializes the convolutional neural network model.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(ConvNet, self).__init__()\n",
    "        # C1: RGB → 6 maps, kernel 5x5 (LeNet uses 5x5; RGB handled via in_channels=3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0)   # 32→28\n",
    "        self.pool  = nn.AvgPool2d(kernel_size=2, stride=2)                                          # 28→14, 10→5\n",
    "        # C3\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)                           # 14→10\n",
    "        # C5 (acts fully connected over 5×5 to 1×1)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)                         # 5→1\n",
    "        # F6 and output\n",
    "        self.fc1   = nn.Linear(120, 84)     # F6\n",
    "        self.fc2   = nn.Linear(84, 100)     # CIFAR-100 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = F.relu(self.conv1(x))           # (N, 6, 28, 28)\n",
    "        x = self.pool(x)                    # (N, 6, 14, 14)\n",
    "        x = F.relu(self.conv2(x))           # (N, 16, 10, 10)\n",
    "        x = self.pool(x)                    # (N, 16, 5, 5)\n",
    "        x = F.relu(self.conv3(x))           # (N, 120, 1, 1)\n",
    "        x = x.view(x.size(0), -1)           # (N, 120)\n",
    "        x = F.relu(self.fc1(x))             # (N, 84)\n",
    "        x = self.fc2(x)                     # (N, 100) logits\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Preparation of Training (7 points)**\n",
    "\n",
    "In this section, you will create a custom dataset class to load the CIFAR-100 data, define a transform function for data augmentation, and set up an optimizer for training. While the previous section utilized the built-in CIFAR-100 class from `torchvision`, in practice, you often need to prepare datasets manually. Here, you will implement the `CIFAR100_loader` class to handle the dataset and use `DataLoader` to make it iterable. You will also define a transform function for data augmentation and an optimizer for updating the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        '''\n",
    "        Initializes the CIFAR-100 dataset loader.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory to store the dataset.\n",
    "            train (bool): If True, loads the training data; otherwise, loads the test data.\n",
    "            transform (callable, optional): The data transformations to apply.\n",
    "            download (bool): If True, downloads the dataset if it is not already available.\n",
    "        '''\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        self.dataset = torchvision.datasets.CIFAR100(\n",
    "            root=root,\n",
    "            train=train,\n",
    "            transform=transform,\n",
    "            download=download\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and label tensors.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transforms():\n",
    "    '''\n",
    "    Creates the data transformations for the CIFAR-100 dataset.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The data transformations for the dataset.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, learning_rate=0.001):\n",
    "    '''\n",
    "    Creates an optimizer for the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Adam: The optimizer for the model.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Training the Networks (5 points)**\n",
    "\n",
    "In this section, you will complete the `train` function and use it to train both the `TwoLayerNet` and `ConvNet` models. You will use the custom `CIFAR100_loader`, transform function, and optimizer function that you implemented. The goal is to compare the performance of the two models on the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, testloader):\n",
    "    '''\n",
    "    Validates the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test dataset.\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f} %')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_class(net, testloader, classes):\n",
    "    '''\n",
    "    Validates the model on the test dataset per class.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        classes (tuple): The tuple of class names.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class_correct = [0. for _ in range(len(classes))]\n",
    "    class_total = [0. for _ in range(len(classes))]\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions = (predicted == labels).squeeze()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += correct_predictions[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f'Accuracy of {class_name:5s} : {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, criterion, optimizer, epochs=100):\n",
    "    '''\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Get already-defined globals (you mentioned these exist)\n",
    "    g = globals()\n",
    "    val_loader = g.get(\"val_loader\", g.get(\"test_loader\", None))\n",
    "    validate_fn = g.get(\"validate\", None)\n",
    "    validate_per_class_fn = g.get(\"validate_per_class\", None)\n",
    "    classes = g.get(\"classes\", None)\n",
    "\n",
    "    # Prepare checkpoint paths\n",
    "    save_dir = os.path.join(os.path.expanduser(\"~\"), \"cv1\", \"outputs\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_ckpt_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "    last_ckpt_path = os.path.join(save_dir, \"last_model.pt\")\n",
    "\n",
    "    print(\"CWD:\", os.getcwd())\n",
    "    print(\"Device:\", device)\n",
    "    print(\"Will save BEST to:\", os.path.abspath(best_ckpt_path))\n",
    "    print(\"Will save LAST to:\", os.path.abspath(last_ckpt_path))\n",
    "\n",
    "    best_val_acc = -1.0  # ensures first epoch saves\n",
    "    print_every = 100\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for step, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            if step % print_every == 0:\n",
    "                now = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "                print(f\"[{now}] epoch {epoch:02d} step {step}/{len(train_loader)} loss={loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = running_loss / max(total, 1)\n",
    "        train_acc = 100.0 * correct / max(total, 1)\n",
    "        msg = f\"Epoch [{epoch}/{epochs}]  loss: {avg_loss:.4f}  train_acc: {train_acc:.2f}%\"\n",
    "\n",
    "        # ---- validation ----\n",
    "        val_acc = None\n",
    "        if val_loader is not None and callable(validate_fn):\n",
    "            val_acc = validate_fn(net, val_loader)  # uses your defined validate()\n",
    "\n",
    "            # optional per-class\n",
    "            if callable(validate_per_class_fn) and classes is not None:\n",
    "                validate_per_class_fn(net, val_loader, classes)\n",
    "\n",
    "            # save best\n",
    "            if epoch == 1 or val_acc >= best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(net.state_dict(), best_ckpt_path)\n",
    "                print(f\"Checkpoint saved: {best_ckpt_path}\")\n",
    "\n",
    "            msg += f\"  val_acc: {val_acc:.2f}%  best_val: {best_val_acc:.2f}%\"\n",
    "        else:\n",
    "            msg += \"  (no validation)\"\n",
    "\n",
    "        print(msg)\n",
    "\n",
    "    # ---- save last checkpoint ----\n",
    "    torch.save(net.state_dict(), last_ckpt_path)\n",
    "    print(f\"Saved LAST to: {os.path.abspath(last_ckpt_path)}\")\n",
    "    if best_val_acc >= 0:\n",
    "        print(f\"BEST validation accuracy: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the datasets and data loaders for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Datasets & loaders\n",
    "train_tf, test_tf = create_transforms()\n",
    "train_dataset = CIFAR100_loader(root=\"./data\", train=True,  transform=train_tf,  download=True)\n",
    "test_dataset  = CIFAR100_loader(root=\"./data\", train=False, transform=test_tf,   download=True)\n",
    "\n",
    "pin = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,  num_workers=2, pin_memory=pin)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=256, shuffle=False, num_workers=2, pin_memory=pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the TwoLayerNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# TwoLayerNet\n",
    "twolayer = TwoLayerNet(input_size=3*32*32, hidden_size=512, num_classes=100)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt1 = create_optimizer(twolayer, learning_rate=1e-3)\n",
    "\n",
    "train(twolayer, train_loader, criterion, opt1, epochs=20)\n",
    "validate(twolayer, test_loader)\n",
    "validate_per_class(twolayer, test_loader, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the ConvNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# ConvNet\n",
    "convnet = ConvNet()\n",
    "opt2 = create_optimizer(convnet, learning_rate=1e-3)\n",
    "\n",
    "train(convnet, train_loader, criterion, opt2, epochs=20)\n",
    "validate(convnet, test_loader)\n",
    "validate_per_class(convnet, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Setting Up the Hyperparameters (14 points)**\n",
    "\n",
    "In this section, you will experiment with both the `ConvNet` and `TwoLayerNet` models by setting up and tuning the hyperparameters to achieve the highest possible accuracy. You have the flexibility to modify the training process, including the `train` function, `DataLoader`, `transform` functions, and optimizer as needed.\n",
    "\n",
    "1. Adjust the hyperparameters, including learning rate, batch size, number of epochs, optimizer, weight decay, and transform function to improve the performance of both networks. Modify the training procedure and architecture as necessary. You can also add components like Batch Normalization layers.\n",
    "2. Add two more layers to both `TwoLayerNet` and `ConvNet`. You can decide the size and placement of these layers. Evaluate if these changes result in higher performance and explain your findings.\n",
    "3. Show the final results and describe the modifications made to enhance performance. Discuss the impact of hyperparameter tuning on both `TwoLayerNet` and `ConvNet`.\n",
    "4. Compare the two networks in terms of architecture, performance, and learning rates. Provide a detailed explanation of the differences observed.\n",
    "\n",
    "**Note:** Do not use external pre-trained networks and limit additional convolutional layers to a maximum of three beyond the original architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Stronger transforms\n",
    "def create_transforms_hp():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "        transforms.RandomErasing(p=0.25, scale=(0.02,0.2), ratio=(0.3,3.3), value=0),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "    ])\n",
    "    return train_tf, test_tf\n",
    "\n",
    "\n",
    "# TwoLayerNet (+2 layers): 3072 -> 1024 -> 512 -> 256 -> 100, with BN/Dropout\n",
    "class TwoLayerNetDeep(nn.Module):\n",
    "    def __init__(self, input_size=3*32*32, num_classes=100, h1=1024, h2=512, h3=256, pdrop=0.3):\n",
    "        super(TwoLayerNetDeep, self).__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1, self.bn1 = nn.Linear(input_size, h1), nn.BatchNorm1d(h1)\n",
    "        self.fc2, self.bn2 = nn.Linear(h1, h2), nn.BatchNorm1d(h2)      # +1 extra hidden\n",
    "        self.fc3, self.bn3 = nn.Linear(h2, h3), nn.BatchNorm1d(h3)      # +2 extra hidden\n",
    "        self.out = nn.Linear(h3, num_classes)\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.drop(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.drop(F.relu(self.bn3(self.fc3(x))))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# ConvNet (+2 layers): extra conv (3x3) + extra FC, BN/Dropout\n",
    "# Flow: C1(3->6,5x5)->pool->C3(6->16,5x5)->C3b(16->32,3x3)->pool->C5(32->120,5x5)->FC(256)->FC(84)->OUT(100)\n",
    "class ConvNetDeep(nn.Module):\n",
    "    def __init__(self, pdrop=0.25):\n",
    "        super(ConvNetDeep, self).__init__()\n",
    "        self.conv1  = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0)   # 32->28\n",
    "        self.bn1    = nn.BatchNorm2d(6)\n",
    "        self.pool   = nn.AvgPool2d(2,2)                                     # 28->14, 10->5\n",
    "        self.conv2  = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)  # 14->10\n",
    "        self.bn2    = nn.BatchNorm2d(16)\n",
    "        self.conv2b = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # 10->10 (extra conv)\n",
    "        self.bn2b   = nn.BatchNorm2d(32)\n",
    "        self.conv3  = nn.Conv2d(32, 120, kernel_size=5, stride=1, padding=0) # 5->1\n",
    "        self.bn3    = nn.BatchNorm2d(120)\n",
    "        self.drop2d = nn.Dropout2d(pdrop)\n",
    "\n",
    "        self.fc1    = nn.Linear(120, 256)\n",
    "        self.bnfc1  = nn.BatchNorm1d(256)\n",
    "        self.fcmid  = nn.Linear(256, 84)   # extra FC\n",
    "        self.bnmid  = nn.BatchNorm1d(84)\n",
    "        self.out    = nn.Linear(84, 100)\n",
    "        self.drop1d = nn.Dropout(pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))           # (N,6,28,28)\n",
    "        x = self.pool(x)                               # (N,6,14,14)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))            # (N,16,10,10)\n",
    "        x = self.drop2d(F.relu(self.bn2b(self.conv2b(x))))  # (N,32,10,10)\n",
    "        x = self.pool(x)                               # (N,32,5,5)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))            # (N,120,1,1)\n",
    "        x = x.view(x.size(0), -1)                      # (N,120)\n",
    "        x = self.drop1d(F.relu(self.bnfc1(self.fc1(x))))    # (N,256)\n",
    "        x = self.drop1d(F.relu(self.bnmid(self.fcmid(x))))  # (N,84)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# Data loaders (use stronger aug)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "train_tf, test_tf = create_transforms_hp()\n",
    "train_dataset = CIFAR100_loader(root=\"./data\", train=True,  transform=train_tf,  download=True)\n",
    "test_dataset  = CIFAR100_loader(root=\"./data\", train=False, transform=test_tf,   download=True)\n",
    "\n",
    "on_cuda = torch.cuda.is_available()\n",
    "pin = on_cuda\n",
    "BATCH_TRAIN = 64 if not on_cuda else 128   # smaller batch for CPU\n",
    "BATCH_TEST  = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_TRAIN, shuffle=True,\n",
    "                          num_workers=4 if on_cuda else 2, pin_memory=pin, persistent_workers=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_TEST, shuffle=False,\n",
    "                          num_workers=4 if on_cuda else 2, pin_memory=pin, persistent_workers=True)\n",
    "\n",
    "# Optimizers & loss (shared)\n",
    "criterion_hp = nn.CrossEntropyLoss(label_smoothing=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of TwoLayerNet after hyperparameter tuning and compare it with the ConvNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "mlp = TwoLayerNetDeep()\n",
    "opt_mlp = optim.AdamW(mlp.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "# Keep epochs modest if you're on CPU; increase if you have GPU\n",
    "EPOCHS_MLP = 10 if not torch.cuda.is_available() else 30\n",
    "train(mlp, train_loader, criterion_hp, opt_mlp, epochs=EPOCHS_MLP)\n",
    "\n",
    "acc_mlp = validate(mlp, test_loader)\n",
    "validate_per_class(mlp, test_loader, classes)\n",
    "print({\"TwoLayerNetDeep_acc\": acc_mlp})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of ConvNet after hyperparameter tuning and compare it with the TwoLayerNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# ==== Train & evaluate the tuned ConvNet (deeper CNN) ====\n",
    "\n",
    "cnn = ConvNetDeep()\n",
    "opt_cnn = optim.AdamW(cnn.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "EPOCHS_CNN = 10 if not torch.cuda.is_available() else 30\n",
    "train(cnn, train_loader, criterion_hp, opt_cnn, epochs=EPOCHS_CNN)\n",
    "\n",
    "acc_cnn = validate(cnn, test_loader)\n",
    "validate_per_class(cnn, test_loader, classes)\n",
    "print({\"ConvNetDeep_acc\": acc_cnn})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)**\n",
    "\n",
    "In this section, you will work with a subset of the [STL-10](https://cs.stanford.edu/~acoates/stl10/) dataset, containing higher resolution images and different object classes than CIFAR-100. Before fine-tuning your ConvNet on this dataset, first complete the `visualise_stl10` function to display sample images from the following 5 classes:\n",
    "\n",
    "1. **Bird**\n",
    "2. **Deer**\n",
    "3. **Dog**\n",
    "4. **Horse**\n",
    "5. **Monkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_stl10(class_mapping):\n",
    "    '''\n",
    "    Visualizes 5 images from each specified class in the STL-10 dataset.\n",
    "\n",
    "    Args:\n",
    "        class_mapping (dict): A dictionary mapping class indices to class names.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class mapping for bird, deer, dog, horse, and monkey\n",
    "class_mapping = {1: 'bird', 4: 'deer', 5: 'dog', 6: 'horse', 7: 'monkey'}\n",
    "\n",
    "# Visualize STL-10 classes\n",
    "visualise_stl10(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the data, implement the `STL10_loader` class to create a custom data loader that initializes the dataset, extracts the target classes, and applies the necessary image transformations. Once these tasks are completed, you will move on to fine-tuning the ConvNet on this dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL10_loader(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        '''\n",
    "        Initializes the STL10 dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): Root directory of the dataset.\n",
    "            train (bool): If True, use the training set, otherwise use the test set.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its target label.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Fine-tuning ConvNet on STL-10 (14 points)**\n",
    "\n",
    "In this section, you will load the pre-trained parameters of the ConvNet (trained on CIFAR-100) and modify the output layer to adapt it to the new dataset containing 5 classes. You can either first load the pre-trained parameters and then modify the output layer, or change the output layer before loading the matched pre-trained parameters. Once modified, you will train the model and document the settings of hyperparameters, accuracy, and learning curve. Additionally, visualize both the training loss and accuracy to assess the learning process. To gain a deeper understanding of the feature learning process, consider using techniques like [**t-sne**](https://lvdmaaten.github.io/tsne/) for feature space visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Bonus Challenge (optional)**\n",
    "\n",
    "Try to achieve the highest possible accuracy on the test dataset (5 classes from STL-10) by adjusting hyperparameters, modifying architectures, or applying techniques like data augmentation. The top-performing teams will earn bonus points that can significantly boost their final lab grade, even allowing it to exceed 10 (up to 11):\n",
    "\n",
    "- **1st place:** +1.0 to the final grade of the final lab\n",
    "- **2nd place:** +0.8 to the final grade of the final lab\n",
    "- **3rd place:** +0.6 to the final grade of the final lab\n",
    "- **4th place:** +0.4 to the final grade of the final lab\n",
    "- **5th place:** +0.2 to the final grade of the final lab\n",
    "\n",
    "**Hint:** You may use techniques like data augmentation, freezing early layers, modifying architecture, or optimizing hyperparameters. Only data from CIFAR-100 and STL-10 can be used, and you cannot add more than 3 additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
